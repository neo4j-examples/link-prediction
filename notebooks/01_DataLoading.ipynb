{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Dataset Loading\n",
    "\n",
    "In this notebook we're going to load the citation dataset into Neo4j.\n",
    "\n",
    "First let's import a couple of Python libraries that will help us with this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing py2neo library which we'll use to import the data into Neo4j. py2neo is a client library and toolkit for working with Neo4j from within Python applications. It is well suited for Data Science workflows and has great integration with other Python Data Science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link-prediction-neo4j:7687\n"
     ]
    }
   ],
   "source": [
    "driver = GraphDatabase.driver(\"bolt://link-prediction-neo4j\", auth=(\"neo4j\", \"admin\"))        \n",
    "print(driver.address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Constraints\n",
    "\n",
    "First let's create some constraints to make sure we don't import duplicate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'constraints_added': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'constraints_added': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'constraints_added': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with driver.session(database=\"neo4j\") as session:\n",
    "    display(session.run(\"CREATE CONSTRAINT ON (a:Article) ASSERT a.index IS UNIQUE\").consume().counters)\n",
    "    display(session.run(\"CREATE CONSTRAINT ON (a:Author) ASSERT a.name IS UNIQUE\").consume().counters)\n",
    "    display(session.run(\"CREATE CONSTRAINT ON (v:Venue) ASSERT v.name IS UNIQUE\").consume().counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Now let's load the data into the database. We'll create nodes for Articles, Venues, and Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record batches=52 total=51956 timeTaken=233 committedOperations=51956 failedOperations=0 failedBatches=0 retries=0 errorMessages={} batch={'total': 52, 'committed': 52, 'failed': 0, 'errors': {}} operations={'total': 51956, 'committed': 51956, 'failed': 0, 'errors': {}} wasTerminated=False failedParams={}>\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  'UNWIND [\"dblp-ref-0.json\", \"dblp-ref-1.json\", \"dblp-ref-2.json\", \"dblp-ref-3.json\"] AS file\n",
    "   CALL apoc.load.json(\"https://github.com/mneedham/link-prediction/raw/master/data/\" + file)\n",
    "   YIELD value WITH value\n",
    "   return value',\n",
    "  'MERGE (a:Article {index:value.id})\n",
    "   SET a += apoc.map.clean(value,[\"id\",\"authors\",\"references\", \"venue\"],[0])\n",
    "   WITH a, value.authors as authors, value.references AS citations, value.venue AS venue\n",
    "   MERGE (v:Venue {name: venue})\n",
    "   MERGE (a)-[:VENUE]->(v)\n",
    "   FOREACH(author in authors | \n",
    "     MERGE (b:Author{name:author})\n",
    "     MERGE (a)-[:AUTHOR]->(b))\n",
    "   FOREACH(citation in citations | \n",
    "     MERGE (cited:Article {index:citation})\n",
    "     MERGE (a)-[:CITED]->(cited))', \n",
    "   {batchSize: 1000, iterateList: true});\n",
    "\"\"\"\n",
    "\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    result = session.run(query)\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nodes_deleted': 132357, 'relationships_deleted': 261202}\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (a:Article) \n",
    "WHERE not(exists(a.title))\n",
    "DETACH DELETE a\n",
    "\"\"\"\n",
    "\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    result = session.run(query)\n",
    "    print(result.consume().counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we'll explore the data that we've imported. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
